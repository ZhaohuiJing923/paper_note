# 2021-11-4
Title:The Cityscapes Dataset for Semantic Urban Scene Understanding
Auther:Marius Cordts
Comments:发表在CVPR2016上的文章，提出了Cityscapes数据集，语义城市场景理解。

他们做了：
1. 创造了最大最丰富的有高质量和粗标注的街景数据
2. 为像素级和实例级的语义标注提出了合理的评估方法
3. 深入分析了他们数据集的特性
4. 在他们的benchmark上测试了众多SOTA方法。

他们声称：
1. 最先进的方法在我们的数据集上的性能相对顺序与在更通用的数据集(如PASCAL VOC)上的性能明显不同。我们的结论是，通过这种通用数据集可能无法实现城市场景理解的重大进展。
2. 目前在KITTI和CamVid上的语义标记的最先进水平很容易达到，甚至在某种程度上通过应用现成的全卷积网络[41]只在城市景观上训练，如第3.5节所示。这突出了我们的数据集的兼容性和独特优势。
3. Cityscapes将给我们的领域带来重大的新挑战，因为它目前还远远没有被解决。


# 2021-11-5
Title：MINIMAL GEOMETRY-DISTORTION CONSTRAINT FOR UNSUPERVISED IMAGE-TO-IMAGE TRANSLATION
Auther：郭家贤
Comments：ICLR2021，主要做了：

image to image translation问题中的几何失真问题，首先把用之前一些方法translation前后的图片像素值绘图，发现一个目标区域中颜色在translation之后可以随机的映射各种颜色，之前方法这种颜色的随机
transformation造成了几何失真。所有，作者提出，减少颜色的随机transformation是减轻I2I中几何失真的一个办法。
提出，minimal geometry-distortion constraint (MGC)，限制在translation过程中保持像素级结构。举个简单例子就是：一张图片中叶子的颜色可以是红色（秋天），也可以是绿色（夏天），但是translation
前后，叶子的位置应该是一致的。这确实可以解决I2I中几何失真问题。
推导rSMI，数学公式很多。

问题：
1.为什么要设计新的MI，已有的MI不能实现吗？

总的来说，作者发现了现有I2I中几何失真的问题，并且分析了可能是translation过程中颜色随机transformation造成该问题，所有，提出MGC，通过rSMI来衡量translation前后图片的信息，以此来限制translation
时保持几何信息。


# 2021-11-8
Title：Unpaired Image-to-Image Translation using Adversarial Consistency Loss
Auther：Yihao Zhao
Comments：ACL-GAN，ECCV2020的一篇文章。主要是提出了一种新的loss用于unpaired I2I。

主要是考虑到cycle-consistency loss的不足：‘due to the strict pixel-level constraint, it cannot perform shape changes, remove large objects, or ignore irrelevant texture.’说主要
问题在于，它假设translated images contain all the information of the input images in order to reconstruct the input images。这导致保留source domain feature，比如大目标的痕迹（眼镜）
纹理（脸上的胡子），不好。此外，cycle-consistency loss约束source domain的形状改变（头发和脸型几乎没有变化）。
我认为它文章的重点是说，想让生成的image包含source domain的信息，从分布的角度，而不是保留pixel-level的一致性。translated image不需要和特定的source domin image像。至于实现上的关键是：acl
loss，判别器判别，(xS; ^xS)和(xS; ~xS)的真假，~xs是从x合成的，多模式输出，说是为了增加可能性。进一步，可以，让判别器关注feature level，即^xs不需要和特定的source image像。


Title：Contrastive Learning for Unpaired Image-to-Image Translation
Auther：Taesung Park
Comments：ECCV2020的文章，CUT，将对比学习思路引入到unpaired I2I中，通过对比学习来最大化输入输出间的互信息。

主要是希望take on the appearance of the target domain (a zebra), while retaining the structure, or content, of the specific input horse。通常，appearance由对抗loss玉树，content是
cycle-consistency保持。但是cycle-consistency，是双向，太严格，所有，提出straightforward的方式，实现保留input的结构信息，同时能有target的appearance信息。
实现：contrastive loss, InfoNCE loss, which aims to learn an embedding or an encoder that associates corresponding patches to each other, while disassociating them from others.
encoder在学习时关注the commonalitiesbetween the two domains, such as object parts and shapes, while being invariant to the differences, such as the textures of the animals.
重点是图2，We set up an (N+1)-way classification problem, where N negative patches are sampled from the same input image at different locations。


# 2021-11-9
Title：SPatchGAN: A Statistical Feature Based Discriminator for Unsupervised Image-to-Image Translation
Auther：Xuning Shao, Weidong Zhang
Comments：这是网易AI lab在ICCV2021的一篇文章。针对unsupervised I2I任务中，提出了关注统计特征还不是单个patch的判别器网络。

研究的出发点：目前的patchgan不能够保证unsupervised I2I网络中稳定性，而为了稳定和提升训练过程，各种约束如cycle-constraint，shared latent space 假设被提出来。这些约束用在生成器上来缓解
模式坍塌等问题。然而，这些方法造成GAN和额外约束的冲突，导致不完全translation。比如图1中给的例子。所以本文，想不为生成器引入更多约束，而稳定判别器自身。所以提出SPatchGAN，利用了统计特征而不
是patch。
它文章分析自己work的点：与基于单个patch的方法不同，它的判别器的输入来源于统计特征。统计特征通常比单个patch更稳定，因为它的每个元素是在HW的patch上计算。不同与单个patch受限的感受野，每个统计
特性是关于整张图的。全局的视野有助于验证形状变形的正确性。


读了以上ACL-GAN，CUT，Cityscapes，MGC，SPatchGAN五篇文章，对我自己的任务有何启发？
我的任务的描述：想要从sharp图中预测的motion flow，既有保持与原图结构上的一致，还能在每个语义内保证flow大小、方向的多样性呢。
思考：
1.为什么要思考这个问题？用youjian的motion offset，从blurry到pre-train的motion offset，是可以训练出来的，不过是得到offset。但是换成blurry中估计flow，就不太行了，loss可以下降，但最好
收敛到1，并且估计的motion flow也不够好。这还是在L1 loss的强约束下。但是我们的目的是要生成多样的motion flow，前一步强约束都效果不好，更不要说用讨论的sharp-flow pair对的GAN loss了。GAN loss
对我的任务来说还是约束比较强，我们希望，结构一致性，然后语义内的flow 的大小、方向都可以多样，所以才有了师兄让我看看郭家贤那篇文章，希望能够找到一些思路。
2.

