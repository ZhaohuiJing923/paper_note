# 2021-11-4
Title:The Cityscapes Dataset for Semantic Urban Scene Understanding
Auther:Marius Cordts
Comments:发表在CVPR2016上的文章，提出了Cityscapes数据集，语义城市场景理解。

他们做了：
1. 创造了最大最丰富的有高质量和粗标注的街景数据
2. 为像素级和实例级的语义标注提出了合理的评估方法
3. 深入分析了他们数据集的特性
4. 在他们的benchmark上测试了众多SOTA方法。

他们声称：
1. 最先进的方法在我们的数据集上的性能相对顺序与在更通用的数据集(如PASCAL VOC)上的性能明显不同。我们的结论是，通过这种通用数据集可能无法实现城市场景理解的重大进展。
2. 目前在KITTI和CamVid上的语义标记的最先进水平很容易达到，甚至在某种程度上通过应用现成的全卷积网络[41]只在城市景观上训练，如第3.5节所示。这突出了我们的数据集的兼容性和独特优势。
3. Cityscapes将给我们的领域带来重大的新挑战，因为它目前还远远没有被解决。


# 2021-11-5
Title：MINIMAL GEOMETRY-DISTORTION CONSTRAINT FOR UNSUPERVISED IMAGE-TO-IMAGE TRANSLATION
Auther：郭家贤
Comments：ICLR2021，主要做了：

image to image translation问题中的几何失真问题，首先把用之前一些方法translation前后的图片像素值绘图，发现一个目标区域中颜色在translation之后可以随机的映射各种颜色，之前方法这种颜色的随机
transformation造成了几何失真。所有，作者提出，减少颜色的随机transformation是减轻I2I中几何失真的一个办法。
提出，minimal geometry-distortion constraint (MGC)，限制在translation过程中保持像素级结构。举个简单例子就是：一张图片中叶子的颜色可以是红色（秋天），也可以是绿色（夏天），但是translation
前后，叶子的位置应该是一致的。这确实可以解决I2I中几何失真问题。
推导rSMI，数学公式很多。

问题：
1.为什么要设计新的MI，已有的MI不能实现吗？

总的来说，作者发现了现有I2I中几何失真的问题，并且分析了可能是translation过程中颜色随机transformation造成该问题，所有，提出MGC，通过rSMI来衡量translation前后图片的信息，以此来限制translation
时保持几何信息。


# 2021-11-8
Title：Unpaired Image-to-Image Translation using Adversarial Consistency Loss
Auther：Yihao Zhao
Comments：ACL-GAN，ECCV2020的一篇文章。主要是提出了一种新的loss用于unpaired I2I。

主要是考虑到cycle-consistency loss的不足：‘due to the strict pixel-level constraint, it cannot perform shape changes, remove large objects, or ignore irrelevant texture.’说主要
问题在于，它假设translated images contain all the information of the input images in order to reconstruct the input images。这导致保留source domain feature，比如大目标的痕迹（眼镜）
纹理（脸上的胡子），不好。此外，cycle-consistency loss约束source domain的形状改变（头发和脸型几乎没有变化）。
我认为它文章的重点是说，想让生成的image包含source domain的信息，从分布的角度，而不是保留pixel-level的一致性。translated image不需要和特定的source domin image像。至于实现上的关键是：acl
loss，判别器判别，(xS; ^xS)和(xS; ~xS)的真假，~xs是从x合成的，多模式输出，说是为了增加可能性。进一步，可以，让判别器关注feature level，即^xs不需要和特定的source image像。


Title：Contrastive Learning for Unpaired Image-to-Image Translation
Auther：Taesung Park
Comments：ECCV2020的文章，CUT，将对比学习思路引入到unpaired I2I中，通过对比学习来最大化输入输出间的互信息。

主要是希望take on the appearance of the target domain (a zebra), while retaining the structure, or content, of the specific input horse。通常，appearance由对抗loss玉树，content是
cycle-consistency保持。但是cycle-consistency，是双向，太严格，所有，提出straightforward的方式，实现保留input的结构信息，同时能有target的appearance信息。
实现：contrastive loss, InfoNCE loss, which aims to learn an embedding or an encoder that associates corresponding patches to each other, while disassociating them from others.
encoder在学习时关注the commonalitiesbetween the two domains, such as object parts and shapes, while being invariant to the differences, such as the textures of the animals.
重点是图2，We set up an (N+1)-way classification problem, where N negative patches are sampled from the same input image at different locations。


# 2021-11-9
Title：SPatchGAN: A Statistical Feature Based Discriminator for Unsupervised Image-to-Image Translation
Auther：Xuning Shao, Weidong Zhang
Comments：这是网易AI lab在ICCV2021的一篇文章。针对unsupervised I2I任务中，提出了关注统计特征还不是单个patch的判别器网络。

研究的出发点：目前的patchgan不能够保证unsupervised I2I网络中稳定性，而为了稳定和提升训练过程，各种约束如cycle-constraint，shared latent space 假设被提出来。这些约束用在生成器上来缓解
模式坍塌等问题。然而，这些方法造成GAN和额外约束的冲突，导致不完全translation。比如图1中给的例子。所以本文，想不为生成器引入更多约束，而稳定判别器自身。所以提出SPatchGAN，利用了统计特征而不
是patch。
它文章分析自己work的点：与基于单个patch的方法不同，它的判别器的输入来源于统计特征。统计特征通常比单个patch更稳定，因为它的每个元素是在HW的patch上计算。不同与单个patch受限的感受野，每个统计
特性是关于整张图的。全局的视野有助于验证形状变形的正确性。


读了以上ACL-GAN，CUT，Cityscapes，MGC，SPatchGAN五篇文章，对我自己的任务有何启发？
我的任务的描述：想要从sharp图中预测的motion flow，既有保持与原图结构上的一致，还能在每个语义内保证flow大小、方向的多样性呢。
思考：
1.为什么要思考这个问题？用youjian的motion offset，从blurry到pre-train的motion offset，是可以训练出来的，不过是得到offset。但是换成blurry中估计flow，就不太行了，loss可以下降，但最好
收敛到1，并且估计的motion flow也不够好。这还是在L1 loss的强约束下。但是我们的目的是要生成多样的motion flow，前一步强约束都效果不好，更不要说用讨论的sharp-flow pair对的GAN loss了。GAN loss
对我的任务来说还是约束比较强，我们希望，结构一致性，然后语义内的flow 的大小、方向都可以多样，所以才有了师兄让我看看郭家贤那篇文章，希望能够找到一些思路。
2.





# 2021-11-10
Title：OpenGAN: Open-Set Recognition via Open Data Generation
Auther：Shu Kong, Deva Ramanan
Comments：这是ICCV021的honorable mentions文章。

主要的出发点在于：他们的motivation是自动驾驶的安全问题。当前的benchmark比如cityscapes在测试是忽略了大量的‘其他’像素，K类之外的东西。结果，一些SOTA的分割方法在训练时也忽略了他们，他们就是
开集的例子。‘其他’中包含了一些重要的目标比如，轮椅，儿童手推车。图示，把儿童手推车/街道市场错分成了摩托车/建筑物。这些错误分类在自动驾驶中造成灾难性错误，因为这些物体需要不同的避障计划(例如，
屈服或减速)。

问题定义：开集识别是什么：
在真实的开放世界中运行的机器学习系统总会遇到与训练列子不同的测试数据，比如在训练中没有观察到的异常或者罕见物体。在K分类中，这个问题可以被表述为开集识别。把属于第K+1类‘其他’类的开集
数据从闭集数据中区分出来。

开集判别的两个idea：基于GAN的，基于OE（异常值暴露）的
一是，用GAN来无监督的学一个闭集数据分布，用它的判别器来作为开集可能性函数。
二是利用一些异常数据作为开集数据，学习一个开或闭得二分类判别器。
前者的不足：是由于GAN训练的不稳定。我们发现，在outlier data的验证集上进行模型选择，GAN判别器确实能够为开集识别提供SOTA方法。所以，opengan直接用GAN判别器作为开集可能性函数。
后者的不足：是过分拟合训练数据中的异常值，不能很好的泛化到其他开集数据中，不可能扩展到开放世界。opengan通过对抗性的假的开集数据来增加训练数据，以此来解决这个问题。
本文OpenGAN，通过结合两者来解决各自的不足。

方法：
开集识别的通用方法包括：首先，开集判别，分类器基于开集可能性函数把样本区分为开集和闭集。然后，在第一步得到的闭集上K分类。核心问题：第一步，开集判别。通常的开集判别假设，没有开集训练数
据。OE证明，异常值暴露，或者在一些异常样本作为开集训练数据来训练的能力，可以提高性能。如图1b。but，这样的分类器过拟合，不能泛化。我们论证，opengan可通过对抗训练地生成器，来生成假的
开集训练样本。
首先训练二分类器D：
判别是来自open/close，公式1，但是，这种方法只有在开集训练样本足够多，能够表示测试的开集数据时才有效，当不能泛化到真实世界会效果差。
合成开集数据：公式2，通过GAN一样，对抗的合成开集数据。但是，训练的极好的G能生成真实的闭集数据，最后使得判别器D对开集判别不合适了。解决如下。
OpenGAN：
用real open&closed-set data and the fake open-data，公式2来优化。在lamda0为0，也就是没有训练的开集数据时，等价正常的GAN，用它的判别器作为开集可能性函数。尽管一些说他们不好
，我们证明在合适的验证集上选择的GAN-discriminator仍然可以SOTA。经由outlier data验证集来进行模型选择，这是本文一大贡献，令lamda0=0时的模型叫OpenGAN-0。
开放测试：
模型选择对GAN来说是挑战。通常情况下，通过对不同模型checkpoint生成的图像进行视觉检查来选择生成器G。我们，必须小心的选择D。试过：用最后一个model的checkpoint或选训练误差最小的一个，但是
都不work，因为对抗训练最后导致D不能判别闭集数据和G生成的开集数据。so，当D在验证集上达到最好的开/闭的分类准确率时，用真实的outlier data验证集来选择D。我们发现这种性能对于outlier data
的验证集是相当稳健的，即使它们来自于与测试时遇到的分布不同的分布。

进一步讨论之前的GAN方法：
判别器 vs 生成器：先前开集识别的工作着眼于用GAN生成真实的开训练图片，用于增广训练集来学一个开集模型，闭集的K分类和开集识别都这样做。我们，没有学单独的开集模型，直接用训练好的判别器为
开集可能性函数。
Features vs. Pixels：GAN典型的生成真实像素级图片。基于GAN的开集识别也生成图。然而，生成高维的真实图是有挑战的，且对开集识别任务不必要。所以，我们通过闭集世界K分类网络学习的OTS特征嵌入来
构建gan。这样就可以“免费”地利用大量的工程工作。
Classification vs. Reconstruction：我们注意到，大多数基于gan的方法在很大程度上依赖于开集识别的重构误差。其基本假设是闭集数据产生的重构误差比开集数据低。尽管似乎有道理，然而，重建复杂
的、高分辨率的image，比如cityscapes中的图，很有挑战。相反，对于开集判别，我们直接使用判别器作为开集似然函数。尽管在一些工作中它作为baseline没有work的很好，但是，它是第一次GAN-
discriminator在各种benchmark上比之前工作要好，多亏了通过开集验证集来模型选择。

实验：
设置1：开集判别需要，把单个数据集划分为开和闭，例如，MINIST数字0-5为闭集用于训练，6-9为开集用于验证。尽管规模小，这是一种常见的针对开集判别的实验方案，用于对开与闭测试示例进行分类。
设置2：开集识别既需要闭集上的k路分类，也需要开集识别。我们用交叉数据集的图像构造开放训练和测试集。
设置3：在语义分割的像素级上检测开集识别，评估像素级开与闭分类精度。
评估指标：为了评估衡量开-闭二元分类性能的开集区分，我们遵循文献[28,35]，并使用ROC曲线下面积(AUROC)。AUROC是一个无校准和无阈值的度量，简化了方法之间的比较，在大型开闭不平衡情况下
是可靠的。对于测量(K+1)-方式分类精度的开放集识别(K闭集类加上(K+1)第1个开放集类)，我们报告了在valsets上所有(K+1)类的宏观平均F1-评分。

对比方法：
baseline：首先，研究传统的在闭训练集上学的生成网络，NN，GMM，都比l2归一化OTS特征表现得好。这两种模型都可以通过阈值NN距离或可能性进行开集识别。进一步研究学一个开闭二分类的OE。最后，
沿用语义分割的传统方法，评估用OE训练的k+1分类器，我们使用softmax得分对应的(K+1)其他类作为开集似然
设置1：

总的来说，它为什么会中：
基于GAN的，假设train只有inlier image；基于OE的，只用到outlier image。本文既用到了outlier image，也用到了生成的fake data。
对抗性合成假的开集数据来增广开集训练数据集，且feature level比pixel效果还要好。证明在open的valset上选择D，直接用GAN-discriminator作为开集可能性函数也是有效的，不依赖reconstruct 
error。


Title：Rethinking Coarse-to-Fine Approach in Single Image Deblurring
Auther：Sung-Jin Cho
Comments：高丽大学，ICCV2021上的一篇文章，主要是设计轻量化的去模糊网络。

他们重新思考了由粗到细的机制，提出了新的叫做多输入多输出的UNet网络。处理多尺度模糊与低计算复杂度。一个encoder-decoder UNet网络。主要是通过SCM，AFF来实现多尺度输入的并入网络和互相融合。
结构：1）一个decoder网络输出多个去模糊图片，简单，可以模拟堆叠子网的传统网络，有粗到细的逐步恢复清晰图像。2）一个encoder网络采用多尺度输入图像。3）AFF（asymmetric feature fusion）
有效的融合多尺度的特征。
评价：轻量化网络，减少了参数量，内存友好：运行速度变快；MIMO—UNet++的去模糊的准确率甚至要好于MPRNet，这点还是很加分的。总的来说，是网络模型的魔改文章，但是能够保持效果就很加分了。
从github上用户的反馈来看，平均的PSNR可以达到论文中所声称的，但是运行时间应该没有论文中声称的高。


# 2021-11-14
Title：Efficient Dynamic Scene Deblurring Using Spatially Variant Deconvolution Network with Optical Flow Guided Training
Auther：Yuan Yuan
Comments：这是CVPR2020的一篇文章，也是着眼于轻量化的去模糊网络。


# 2021-11-16
Title：Adversarially Learned One-Class Classifier for Novelty Detection
Auther：Mohammad Sabokrou, Mohammad Khalooei, Mahmood Fathy, Ehsan Adeli
Comments：CVPR2018的一篇文章，用GAN来解决异常检测。

Contribution：
1.提出了一种用于单类分类器学习的端到端深度网络。据我们所知，本文是第一批介绍用于单类分类的端到端网络的文章之一。
2. 在[31]文献中，几乎所有基于gan的方法都在训练后丢弃生成器或鉴别器(类似于R和D)。只使用其中一个训练过的模型，而我们的设置更有效，并受益于两个训练过的模块，以便在测试阶段进行协作。
3.我们的体系结构在完全没有任何来自新奇类的训练样本的情况下学习模型，并在不同的应用中实现了最先进的性能，如图像中的异常点检测和视频中的异常事件检测。

总的来说，是一种基于GAN的异常检测方法，R在train时，算加上noise的图与原图的重建loss，可以得到假的图，然后让D来判别，这样，可以学到所有正样本空间的概念表示。然后D就可以用来区分正样本和异常了。
R将成为专家，以最小的误差从目标类中重构样本，成功愚弄D。R增强了inliers(来自目标类的样本)，同时对离群值进行析构或抽取，使鉴别器更容易从庞大的inliers池中分离离群值。图1的例子就说明的这个意思。
之后的人所说，这种基于GAN的方法不足在于GAN的训练是不稳定的。


# 2021-11-18
Title：Efficient Spatio-Temporal Recurrent Neural Network for Video Deblurring
Auther：Zhihang Zhong, Ye Gao, Yinqiang Zheng, and Bo Zheng
Comments：是ECCV2020的一篇文章，做的是video deblur。ESTRNN，PSNR：31.07，SSIM：0.9023

提出加一个时空的RNN，STRNN。网络结构和思路都很清晰。考虑到GoPro等合成的模糊图是否能够反映真实场景的模糊，新建了Beam-Splitter Dataset (BSD)数据集，使用带有两个同步摄像头的分束系统得到BSD。
1.a novel RNN-based method for more computational efficient video deblurring. Residual dense block was adopted to the RNN cell to generate hierarchical features from current
frame for better restoration.
2.utilized the global spatio-temporal fusion module for fusing the effective components of hierarchical features from past and future frames.


Title：Cascaded Deep Video Deblurring Using Temporal Sharpness Prior
Auther：Jinshan Pan, Haoran Bai, and Jinhui Tang
Comments：南京理工大学，潘金山组的文章，CVPR2020的文章。CDVD-TSP，PSNR：31.67，SSIM：0.9279。

他们组从18年起，一直在做low-level的工作，图像去模糊，去雾，去噪，视频的去模糊等都有所涉及，并且产出也很好。
1. a simple and compact deep CNN model that simultaneously estimates the optical flow and latent frames for video deblurring.
2. To better explore the properties of consecutive frames, we develop a temporal sharpness prior to constrain deep CNN models.
预训练的PWCNet得到两帧间光流，光流可以可以作为简单的模糊核，作用到清晰图上，产生模糊图。知道了模糊图和光流，同样可以反向计算出清晰图，本文采取了这个思路。
我通过这篇文章，学到，已知光流，如何从清晰图得到模糊图。

